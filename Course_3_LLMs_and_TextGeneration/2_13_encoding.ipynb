{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:51:31.285105Z",
     "start_time": "2025-03-12T14:51:31.276670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "if 'A306709' in os.environ['USERNAME']:\n",
    "    print(\"Running on Christophs computer: update proxy settings.\")\n",
    "    os.environ[\"http_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "    os.environ[\"https_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "else:\n",
    "    print(\"Running on any computer but not Christophs: don't update any proxy settings.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Christophs computer: update proxy settings.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-12T14:51:31.874408Z",
     "start_time": "2025-03-12T14:51:31.869100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# TODO: Feel free to add other imports as needed"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Steps\n",
    "\n",
    "In this exercise, you'll code your own tokenizer from scratching using base\n",
    "Python!\n",
    "\n",
    "You might normally start with a pretrained tokenizer, but this exercise will\n",
    "help you get to know see some of the tokenization steps better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Sample Text\n",
    "\n",
    "Let's first define some sample text you will use to test your tokenization\n",
    "steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T14:51:45.696048Z",
     "start_time": "2025-03-12T14:51:45.689183Z"
    }
   },
   "source": [
    "sample_text = '''Mr. Louis continued to say, \"Penguins are important, \n",
    "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
    "'''\n",
    "\n",
    "print(sample_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Louis continued to say, \"Penguins are important, \n",
      "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "This step is where you'll normalize your text by converting to lowercase,\n",
    "removing accented characters, etc.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalape単o dip?\n",
    "```\n",
    "might be normalized to:\n",
    "```\n",
    "did uncle max like the jalapeno dip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:33.616258Z",
     "start_time": "2025-03-12T15:28:33.611013Z"
    }
   },
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "    remove_chars = \"'\\n\\\":.,!\\n\"\n",
    "    result = text.lower()\n",
    "    for c in remove_chars:\n",
    "        result = result.replace(c, \"\")\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:34.150539Z",
     "start_time": "2025-03-12T15:28:34.145894Z"
    }
   },
   "source": [
    "# Test out your normalization\n",
    "normalize_text(sample_text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mr louis continued to say penguins are important but we mustnt forget the nuumber 1 priority the reader'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretokenization\n",
    "\n",
    "This step will take in the normalized text and pretokenize the text into a list\n",
    "of smaller pieces.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalape単o dip?\n",
    "```\n",
    "might be normalized & then pretokenized to:\n",
    "```\n",
    "[\n",
    "    'did',\n",
    "    'uncle',\n",
    "    'max',\n",
    "    'like',\n",
    "    'the',\n",
    "    'jalapeno',\n",
    "    'dip?',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:35.235919Z",
     "start_time": "2025-03-12T15:28:35.231833Z"
    }
   },
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # TODO: Pretokenize normalized text\n",
    "    return text.split()\n"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:35.750225Z",
     "start_time": "2025-03-12T15:28:35.745152Z"
    }
   },
   "source": [
    "# Test out your pretokenization step (after normalizing the text)\n",
    "normalized_text = normalize_text(sample_text)\n",
    "pretokenize_text(normalized_text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'louis',\n",
       " 'continued',\n",
       " 'to',\n",
       " 'say',\n",
       " 'penguins',\n",
       " 'are',\n",
       " 'important',\n",
       " 'but',\n",
       " 'we',\n",
       " 'mustnt',\n",
       " 'forget',\n",
       " 'the',\n",
       " 'nuumber',\n",
       " '1',\n",
       " 'priority',\n",
       " 'the',\n",
       " 'reader']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This step will take in the list of pretokenized pieces (after the text has \n",
    "been normalized) into the tokens that will be used.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalape単o dip?\n",
    "```\n",
    "might be normalized, pretokenized, and then tokenized to:\n",
    "```\n",
    "[\n",
    "    'did',\n",
    "    'uncle',\n",
    "    'max',\n",
    "    'like',\n",
    "    'the',\n",
    "    'jalapeno',\n",
    "    'dip'\n",
    "    '?',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:36.733030Z",
     "start_time": "2025-03-12T15:28:36.728623Z"
    }
   },
   "source": [
    "# Combine normalization and pretokenization steps before breaking things further\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    # Apply normalization & pretokenization steps \n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: list[str] = pretokenize_text(normalized_text)\n",
    "    # TODO: Go through pretokenized text to create a list of tokens\n",
    "    #tokens = list()\n",
    "    return pretokenized_text"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:37.303091Z",
     "start_time": "2025-03-12T15:28:37.297231Z"
    }
   },
   "source": [
    "# Test out your tokenization (that uses normalizing & pretokenizing functions)\n",
    "tokenize_text(sample_text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'louis',\n",
       " 'continued',\n",
       " 'to',\n",
       " 'say',\n",
       " 'penguins',\n",
       " 'are',\n",
       " 'important',\n",
       " 'but',\n",
       " 'we',\n",
       " 'mustnt',\n",
       " 'forget',\n",
       " 'the',\n",
       " 'nuumber',\n",
       " '1',\n",
       " 'priority',\n",
       " 'the',\n",
       " 'reader']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "This final step will take in the list of tokens from the original text and add\n",
    "any special tokens to the text.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalape単o dip?\n",
    "```\n",
    "might be normalized, pretokenized, and then tokenized to:\n",
    "```\n",
    "[\n",
    "    '[BOS]',\n",
    "    'did',\n",
    "    'uncle',\n",
    "    'max',\n",
    "    'like',\n",
    "    'the',\n",
    "    'jalapeno',\n",
    "    'dip'\n",
    "    '?',\n",
    "    '[EOS]',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:38.932861Z",
     "start_time": "2025-03-12T15:28:38.928785Z"
    }
   },
   "source": [
    "def postprocess_tokens(tokens: list[str]) -> list[str]:\n",
    "    # TODO: Add beginning and end of sequence tokens to your tokenized text\n",
    "    # Can use a format like '[BOS]' & '[EOS]'\n",
    "    updated_tokens = ['[BOS]'] + tokens + ['[EOS]']\n",
    "    return updated_tokens"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:28:39.565151Z",
     "start_time": "2025-03-12T15:28:39.560344Z"
    }
   },
   "source": [
    "# Test full pipeline (normalizing, pretokenizing, tokenizing, & postprocessing)\n",
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "\n",
    "print(tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'mr', 'louis', 'continued', 'to', 'say', 'penguins', 'are', 'important', 'but', 'we', 'mustnt', 'forget', 'the', 'nuumber', '1', 'priority', 'the', 'reader', '[EOS]']\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding & Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Text to Token IDs\n",
    "\n",
    "Create an encoder (`encode()`) that will encode the token strings to integer IDs\n",
    "by defining how to map each token to a unique ID.\n",
    "\n",
    "> HINT: \n",
    "> \n",
    "> An easy method is to assign an arbitrary integer to each unique token from \n",
    "> the corpus by iterating through the unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:30:19.279633Z",
     "start_time": "2025-03-12T15:30:19.274751Z"
    }
   },
   "source": [
    "# Sample corpus (normally this would be much bigger)\n",
    "sample_corpus = (\n",
    "    '''Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn't forget the nuumber 1 priority: the READER!\"''',\n",
    "    '''BRUTUS:\\nHe's a lamb indeed, that baes like a bear.''',\n",
    "    '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\A306709\\AppData\\Local\\Temp\\ipykernel_12052\\2830881065.py:5: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:32:59.213289Z",
     "start_time": "2025-03-12T15:32:59.207020Z"
    }
   },
   "source": [
    "# TODO: Create an encoder to transform token strings to IDs using the sample\n",
    "# corpus as the basis of your encoding\n",
    "\n",
    "# TODO: Your code here (might be outside of the encode() function scope)\n",
    "sample_tokens = {'[BOS]', '[EOS]'}\n",
    "for sample in sample_corpus:\n",
    "    # funktioniert nicht, aber warum ?? sample_tokens.add(token for token in tokenize_text(sample))\n",
    "    tokens = tokenize_text(sample)\n",
    "    for token in tokens:\n",
    "        sample_tokens.add(token)\n",
    "    #sample_tokens = sample_tokens\n",
    "print(sample_tokens)\n",
    "token2id = {}\n",
    "id2token = {}\n",
    "current_id = 0\n",
    "for token in sample_tokens:\n",
    "    token2id[token] = current_id\n",
    "    id2token[current_id] = token\n",
    "    current_id += 1\n",
    "#print(token2id)\n",
    "#print(id2token)\n",
    "\n",
    "def encode(tokens: list[str]) -> list[int]:\n",
    "    # TODO: Complete this function to encode tokens to integer IDs\n",
    "    encoded_tokens = [token2id[x] for x in tokens]\n",
    "    return encoded_tokens\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bear', 'lamb', 'he', 'other', 'the', 'affections', 'counselloris', 'how', 'both', 'are', 'indeed', 'forget', 'priority', 'many', 'important', 'secret', '[BOS]', 'but', 'myself', 'himself', 'close', 'not', 'nuumber', 'will', '[EOS]', 'to', 'louis', 'say', 'like', 'his', 'true--but', 'a', 'that', '1', 'himself--i', 'mr', 'penguins', 'by', 'we', 'and', 'so', 'friends\\\\mbut', 'baes', 'mustnt', 'continued', 'reader', 'brutushes', 'own'}\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `encode()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:33:00.816790Z",
     "start_time": "2025-03-12T15:33:00.811026Z"
    }
   },
   "source": [
    "# Use sample text for testing\n",
    "sample_text = sample_corpus[0]\n",
    "# Create tokens (to be fed to encode())\n",
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "print(f'Tokens:\\n{tokens}\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['[BOS]', 'mr', 'louis', 'continued', 'to', 'say', 'penguins', 'are', 'important', 'but', 'we', 'mustnt', 'forget', 'the', 'nuumber', '1', 'priority', 'the', 'reader', '[EOS]']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:33:02.480422Z",
     "start_time": "2025-03-12T15:33:02.475688Z"
    }
   },
   "source": [
    "# Test encode()\n",
    "encoded_tokens = encode(tokens)\n",
    "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens:\n",
      "[16, 35, 26, 44, 25, 27, 36, 9, 14, 17, 38, 43, 11, 4, 22, 33, 12, 4, 45, 24]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Token IDs to Text\n",
    "\n",
    "Based on your enocder you created (`encode()`), create a decoder (`decode()`) to\n",
    "take a list of token IDs and map them to their associated token."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:33:04.983893Z",
     "start_time": "2025-03-12T15:33:04.979944Z"
    }
   },
   "source": [
    "# TODO: Create an encoder to transform IDs (from encode()) to token strings\n",
    "\n",
    "def decode(ids: list[int]) -> list[str]:\n",
    "    # TODO: Complete this function to decode integer IDs to token strings\n",
    "    token_strings = [id2token[x] for x in ids]\n",
    "    return token_strings"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `decode()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:33:07.580234Z",
     "start_time": "2025-03-12T15:33:07.574720Z"
    }
   },
   "source": [
    "# Use sample text for testing\n",
    "sample_text = sample_corpus[0]\n",
    "# Create tokens\n",
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "print(f'Tokens:\\n{tokens}\\n')\n",
    "\n",
    "# Create token IDs (to be fed to decode())\n",
    "encoded_tokens = encode(tokens)\n",
    "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['[BOS]', 'mr', 'louis', 'continued', 'to', 'say', 'penguins', 'are', 'important', 'but', 'we', 'mustnt', 'forget', 'the', 'nuumber', '1', 'priority', 'the', 'reader', '[EOS]']\n",
      "\n",
      "Encoded Tokens:\n",
      "[16, 35, 26, 44, 25, 27, 36, 9, 14, 17, 38, 43, 11, 4, 22, 33, 12, 4, 45, 24]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:33:08.614691Z",
     "start_time": "2025-03-12T15:33:08.609943Z"
    }
   },
   "source": [
    "# Test out decode()\n",
    "decoded_tokens = decode(encoded_tokens)\n",
    "print(f'Decoded Tokens:\\n{decoded_tokens}\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Tokens:\n",
      "['[BOS]', 'mr', 'louis', 'continued', 'to', 'say', 'penguins', 'are', 'important', 'but', 'we', 'mustnt', 'forget', 'the', 'nuumber', '1', 'priority', 'the', 'reader', '[EOS]']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 116
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
