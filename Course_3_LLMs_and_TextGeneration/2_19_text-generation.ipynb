{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:17:47.059414Z",
     "start_time": "2025-03-12T16:17:47.051638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "if 'A306709' in os.environ['USERNAME']:\n",
    "    print(\"Running on Christophs computer: update proxy settings.\")\n",
    "    os.environ[\"http_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "    os.environ[\"https_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "else:\n",
    "    print(\"Running on any computer but not Christophs: don't update any proxy settings.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Christophs computer: update proxy settings.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:17:47.812164Z",
     "start_time": "2025-03-12T16:17:47.804656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:17:49.149034Z",
     "start_time": "2025-03-12T16:17:49.133621Z"
    }
   },
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:18:19.851711Z",
     "start_time": "2025-03-12T16:18:19.840861Z"
    }
   },
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = 'data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:21:55.329729Z",
     "start_time": "2025-03-12T16:21:55.324794Z"
    }
   },
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "    return text.lower()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:21:56.351434Z",
     "start_time": "2025-03-12T16:21:56.346269Z"
    }
   },
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us kill him, and we'll have corn at our own price.\n",
      "is't a verdict?\n",
      "\n",
      "all:\n",
      "no more talking on't; let it be done: away, away!\n",
      "\n",
      "second citizen:\n",
      "one word, good citizens.\n",
      "\n",
      "first citizen:\n",
      "we are accounted poor\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:23:07.564086Z",
     "start_time": "2025-03-12T16:23:07.560746Z"
    }
   },
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # TODO: Pretokenize normalized text into character strings\n",
    "    return [c for c in text]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:23:08.238337Z",
     "start_time": "2025-03-12T16:23:08.233444Z"
    }
   },
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:23:27.425241Z",
     "start_time": "2025-03-12T16:23:27.420684Z"
    }
   },
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:23:28.097009Z",
     "start_time": "2025-03-12T16:23:28.092056Z"
    }
   },
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:24:38.765813Z",
     "start_time": "2025-03-12T16:24:38.754783Z"
    }
   },
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:25:36.776083Z",
     "start_time": "2025-03-12T16:25:36.769507Z"
    }
   },
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 50,086 characters\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:25:39.012738Z",
     "start_time": "2025-03-12T16:25:39.006804Z"
    }
   },
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:25:55.577914Z",
     "start_time": "2025-03-12T16:25:55.566161Z"
    }
   },
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:27:06.215039Z",
     "start_time": "2025-03-12T16:27:06.210746Z"
    }
   },
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:35:10.490053Z",
     "start_time": "2025-03-12T16:28:21.106673Z"
    }
   },
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.5301671769291447\n",
      "[00m 15.9s (0 0.0) 2.1864]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bedy doy ethinus,\n",
      "none onth hea fhind kinguc's thers, dher there weral, the mawerte.\n",
      "\n",
      "vier:\n",
      "wrant, apk\n",
      "Epoch 2/25, Loss: 2.1812583895917896\n",
      "[00m 31.5s (1 4.0) 1.9870]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be weve hame stem the i crourcius,\n",
      "arcis,\n",
      "cir.\n",
      "ste fhakn wor\n",
      "huce theel whey al moire come\n",
      "at boment p\n",
      "Epoch 3/25, Loss: 2.078537766087931\n",
      "[00m 46.7s (2 8.0) 1.8850]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to berth brut,\n",
      "fot heverh serseng\n",
      "leend. and ofpor nais iais and of voitn maning of frenead,\n",
      "whingy whron\n",
      "Epoch 4/25, Loss: 2.019213487317387\n",
      "[01m 2.5s (3 12.0) 1.8197]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beatlive theer the the ceplinenus:\n",
      "with, thy comnogh arous,\n",
      "uly sutar:\n",
      "hinsud,\n",
      "oud in my citeng\n",
      "the le\n",
      "Epoch 5/25, Loss: 1.9766919331809583\n",
      "[01m 18.2s (4 16.0) 1.7796]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet ontirs, athere'd poods\n",
      "its, the gor,\n",
      "now nor, woud othelq suedes ary bessill\n",
      "of whing, i dem grial\n",
      "Epoch 6/25, Loss: 1.9433630078745345\n",
      "[01m 33.7s (5 20.0) 1.7546]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to ben a, an ther; coug, thow nops loius to sof chaverove use,\n",
      "barwita'l:\n",
      "thests inder, the po fort;\n",
      "whe'\n",
      "Epoch 7/25, Loss: 1.9164595499587136\n",
      "[01m 49.5s (6 24.0) 1.7339]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be dowieshing, is, casgrerours avender non nop ones trot or thon'd hamcio hospee\n",
      "chather a ap ca part\n",
      "\n",
      "Epoch 8/25, Loss: 1.8943748112303762\n",
      "[02m 5.0s (7 28.000000000000004) 1.7176]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bead's canm asseld\n",
      "wuelfort\n",
      "mang pair to renter:\n",
      "all one i he come; the goo? or worther all\n",
      "ot, mis no\n",
      "Epoch 9/25, Loss: 1.875707380002299\n",
      "[02m 20.8s (8 32.0) 1.7068]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beigh' do but,\n",
      "ciqkep, lasy more ereperears! plenily toges cait\n",
      "sutnolnee of the strand of he halglad \n",
      "Epoch 10/25, Loss: 1.859731418560869\n",
      "[02m 39.5s (9 36.0) 1.6993]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be itses coucthe they the folddt fur bus or whatt and work\n",
      "at in the d'ded it gensby worthe,\n",
      "him: thy \n",
      "Epoch 11/25, Loss: 1.845859425745833\n",
      "[02m 55.2s (10 40.0) 1.6941]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beires,\n",
      "got.\n",
      "\n",
      "firgonde'ss pleath with canchhey btem,\n",
      "and griest he reatt one ass trutu, sty is paigh i\n",
      "Epoch 12/25, Loss: 1.8337552776732764\n",
      "[03m 12.7s (11 44.0) 1.6896]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beirees for mis him but free in\n",
      "therhon is betitel--like the somin i jame, thtime; i wand\n",
      "divesenenius\n",
      "Epoch 13/25, Loss: 1.8230699148421852\n",
      "[03m 30.4s (12 48.0) 1.6847]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bets sto, liase ard\n",
      "now atts the us\n",
      "and man ageates you,\n",
      "that dod and to of ome:\n",
      "'the sitthonahed pove\n",
      "Epoch 14/25, Loss: 1.8135556296418651\n",
      "[03m 47.0s (13 52.0) 1.6784]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bererdeces\n",
      "apinpets\n",
      "hereqippicer:\n",
      "say tfo the's cull to marcius,\n",
      "be ous ome my the him peterd enerseai\n",
      "Epoch 15/25, Loss: 1.805094657462245\n",
      "[04m 2.5s (14 56.00000000000001) 1.6712]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be mave both that:\n",
      "but\n",
      "thour 'failea, the mamy?\n",
      "\n",
      "laphey, no drom?\n",
      "\n",
      "senots and comm not of that\n",
      "of mock\n",
      "Epoch 16/25, Loss: 1.7974912845288602\n",
      "[04m 18.7s (15 60.0) 1.6645]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be groonins'd\n",
      "soy\n",
      "thous soy wiser worle dived here\n",
      "have hatius:\n",
      "and peesley that proverot hames\n",
      "a se; \n",
      "Epoch 17/25, Loss: 1.7906103253745422\n",
      "[04m 34.8s (16 64.0) 1.6590]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet to plever:\n",
      "for of shale. are antryomes he hace thoper whand tume.\n",
      "\n",
      "mand he reves offer of thap it \n",
      "Epoch 18/25, Loss: 1.7843518120031387\n",
      "[04m 51.8s (17 68.0) 1.6552]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be word thered tond and teon dey the riment\n",
      "flances come hundisgains, of if to nor the poppy flany the\n",
      "Epoch 19/25, Loss: 1.7786252209172844\n",
      "[05m 8.6s (18 72.0) 1.6521]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to betory?\n",
      "\n",
      "secondty for to their the faid to valees\n",
      "mand: would elderitigtin in that canmanging,\n",
      "genyigh\n",
      "Epoch 20/25, Loss: 1.773372435569763\n",
      "[05m 25.1s (19 76.0) 1.6488]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be he'll do 'til to evercigited and af to.\n",
      "\n",
      "menate nods the, wellan: 'tis heartie and cane'n?\n",
      "\n",
      "marcius\n",
      "Epoch 21/25, Loss: 1.7685356053300558\n",
      "[05m 43.2s (20 80.0) 1.6451]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belon tonour of de in letch, gange,\n",
      "and tougts it let!\n",
      "\n",
      "vilple\n",
      "a, them'd here we and howe if theale, u\n",
      "Epoch 22/25, Loss: 1.7640523651537423\n",
      "[05m 59.5s (21 84.0) 1.6418]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belves ond brathers\n",
      "is, he secouch of not.\n",
      "\n",
      "vongeed feally's: thouth herqomiunt,\n",
      "ict for btonly erever\n",
      "Epoch 23/25, Loss: 1.7598621329560447\n",
      "[06m 15.6s (22 88.0) 1.6392]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be usonfer wellor the bleest, afee\n",
      "with, he hars thy to your they netguth were dive my he but is him; \n",
      "Epoch 24/25, Loss: 1.7559759980954301\n",
      "[06m 33.0s (23 92.0) 1.6372]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besor: i test.\n",
      "\n",
      "meating of it heretited heeste mances to my the to thou up the for their may fo peir o\n",
      "Epoch 25/25, Loss: 1.752377762581213\n",
      "[06m 49.1s (24 96.0) 1.6360]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bear and he gover\n",
      "to such deas, thoughy curgioce has poper defup,\n",
      "them mereeps.\n",
      "\n",
      "marcius geot's in bav\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:38:09.974832Z",
     "start_time": "2025-03-12T16:38:09.724850Z"
    }
   },
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to betold marcius\n",
      "come.\n",
      "\n",
      "unth thecius:\n",
      "what, done geak the mens afrumus,\n",
      "to done.\n",
      "\n",
      "fill onk\n",
      "say scolding \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenizer to tokenize the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:42:00.458744Z",
     "start_time": "2025-03-12T16:41:49.292868Z"
    }
   },
   "source": [
    "# TODO: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "# model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'xlm-roberta-base',\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:42:15.168069Z",
     "start_time": "2025-03-12T16:42:15.095572Z"
    }
   },
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:42:56.603946Z",
     "start_time": "2025-03-12T16:42:56.597943Z"
    }
   },
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 14,374 tokens\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:43:00.978972Z",
     "start_time": "2025-03-12T16:43:00.974696Z"
    }
   },
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:43:15.213822Z",
     "start_time": "2025-03-12T16:43:15.207314Z"
    }
   },
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:43:19.408492Z",
     "start_time": "2025-03-12T16:43:19.401900Z"
    }
   },
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:48:07.365070Z",
     "start_time": "2025-03-12T16:43:31.079507Z"
    }
   },
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.739584188949823\n",
      "[00m 11.5s (0 0.0) 5.8395]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be drum elementses know clubs No Well.le he broughtwordCIN BRU has mouth e buy MAR. come no but That and be conteed with done\n",
      "Epoch 2/25, Loss: 6.093517935355681\n",
      "[00m 22.4s (1 4.0) 5.2964]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be., for,mos ofted-- flatmb where, The this pair be liry that he will putlye I strong abundaTUS\n",
      "Epoch 3/25, Loss: 5.770365263677652\n",
      "[00m 33.1s (2 8.0) 4.8581]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be in my war shallthink nowders him loved,, andture is enough be Wednesdayrry true, We nor ear, say TI isius thought may\n",
      "Epoch 4/25, Loss: 5.478529153794117\n",
      "[00m 44.0s (3 12.0) 4.5978]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beCor VAL Tu Citizen: fier rob And, grim's! fear sir: , wine In ever ever she repliedtisiusunting and w variable\n",
      "Epoch 5/25, Loss: 5.236013997636553\n",
      "[00m 55.5s (4 16.0) 4.3753]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be as hes; If Iu himselfclam peaces not So tenck himffin. C Three like toirship and non wellce you.\n",
      "Epoch 6/25, Loss: 5.032156262472106\n",
      "[01m 6.5s (5 20.0) 4.1854]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beed the appear? A Romeiola whoble content in, the gone deliver great blood faith,lay, away fli. lives ale. but Of BRU\n",
      "Epoch 7/25, Loss: 4.858166806681916\n",
      "[01m 17.4s (6 24.0) 4.0306]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be la de eyes made mayos; and the MAR take Go you Should. withoutINI Corlier; or au and 'tisy in the\n",
      "Epoch 8/25, Loss: 4.705722693611094\n",
      "[01m 28.2s (7 28.000000000000004) 3.9079]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be andren Marcius cheapga fit Invests of think ish. MENENIUS Citizen: Forstruct beland of whitee lovedst the\n",
      "Epoch 9/25, Loss: 4.569167964442536\n",
      "[01m 39.3s (8 32.0) 3.8072]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be marketdle. friends,', their heartownedst'd face, that' my sont the least- Corioliwss for a\n",
      "Epoch 10/25, Loss: 4.444805638561801\n",
      "[01m 50.5s (9 36.0) 3.7198]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be report; Holding theyhout. MENENIUS: Re will before hisside as that for thy hat news spurto nor madam. MENENI\n",
      "Epoch 11/25, Loss: 4.330439859084404\n",
      "[02m 1.5s (10 40.0) 3.6417]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to bept morning To carry more in the? MENENIUS: The ye is thank. In he gives proceed, has to strid press flat 'Tis\n",
      "Epoch 12/25, Loss: 4.224621556118496\n",
      "[02m 12.8s (11 44.0) 3.5709]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be Fur hunal threat Trust face, and they send to those he make man not theius; I it. MENENIUS: True shower call the\n",
      "Epoch 13/25, Loss: 4.126132297622069\n",
      "[02m 24.4s (12 48.0) 3.5058]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beheld to him consul speak's for actionest as which a hot man that aftery! pri: such shouldpt usy super action. they\n",
      "Epoch 14/25, Loss: 4.033973904123285\n",
      "[02m 35.4s (13 52.0) 3.4454]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be made out off Hector hate to'st to yourgg that you are at upon for their own: The counsel gar: heed As us the\n",
      "Epoch 15/25, Loss: 3.9474160554414337\n",
      "[02m 46.2s (14 56.00000000000001) 3.3880]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to bepot, that is as speak toild. See, sir:ech make for answer ande yet'll of it will O, it is Marc\n",
      "Epoch 16/25, Loss: 3.8657528169967548\n",
      "[02m 56.9s (15 60.0) 3.3320]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to belikeous To held, why friends, it is; If I's death and to him woundeded, I' most wars some Marc\n",
      "Epoch 17/25, Loss: 3.7884661280498206\n",
      "[03m 7.8s (16 64.0) 3.2787]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be atre. SICIN All: He where till and what this suit as Than most blood, and prinaves appetit consul, With flight. Your\n",
      "Epoch 18/25, Loss: 3.715185517988651\n",
      "[03m 18.6s (17 68.0) 3.2289]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be aty. BRUTUS: Sir, miss tro with you to ladies but bear For it Make you, if I'll for it as former\n",
      "Epoch 19/25, Loss: 3.645580005539552\n",
      "[03m 30.2s (18 72.0) 3.1824]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be their country that then. AUFIDIUS: the fire. times, and twoed, ' better we these friends; her I'll both\n",
      "Epoch 20/25, Loss: 3.5793732534803633\n",
      "[03m 41.3s (19 76.0) 3.1399]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be office hist at. SICINIUS:ru, evenwihungs; and two super To Con when that! the shapes of yourties\n",
      "Epoch 21/25, Loss: 3.5164986134637437\n",
      "[03m 52.4s (20 80.0) 3.1005]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be many the utal depende of myzed is woundeds mind, Even out best: I well notal day nothingus warsever so\n",
      "Epoch 22/25, Loss: 3.456692656323745\n",
      "[04m 3.3s (21 84.0) 3.0623]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be? MENENIUS: You upon- Patidies. De Hangunds on were BRUTUS: He that hate! not Withers, and Your\n",
      "Epoch 23/25, Loss: 3.3996811821094335\n",
      "[04m 14.4s (22 88.0) 3.0255]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to beto's, which AUFIDIUS: Foolours these And feeur in the cityighs himself their prosper grave of Betime\n",
      "Epoch 24/25, Loss: 3.345317949961979\n",
      "[04m 25.3s (23 92.0) 2.9903]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be silent; and fan better so no choice with a mor honour--if's about whenGI that fear you have done Marcius. What\n",
      "Epoch 25/25, Loss: 3.2934108470755326\n",
      "[04m 36.2s (24 96.0) 2.9556]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to becur him in discover in hearing: and' thedy Three like upon in my hour, and did human cy's the patrilly less Than flag\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:52:45.621308Z",
     "start_time": "2025-03-12T16:52:45.572019Z"
    }
   },
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=0.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be content and pettye: and they are almost thorough; and they are set down o' the people, and did Ret noble ladies,\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
