{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvBHYcpd4v_1"
   },
   "source": [
    "# System setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8pzPvghP5TBs",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:14.919873Z",
     "start_time": "2025-03-11T20:20:14.114967Z"
    }
   },
   "source": [
    "import requests\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:14.931228Z",
     "start_time": "2025-03-11T20:20:14.925879Z"
    }
   },
   "source": [
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "import os\n",
    "if 'A306709' in os.environ['USERNAME']:\n",
    "    print(\"Running on Christophs computer: update proxy settings.\")\n",
    "    os.environ[\"http_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "    os.environ[\"https_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "else:\n",
    "    print(\"Running on any computer but not Christophs: don't update any proxy settings.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Christophs computer: update proxy settings.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.187916Z",
     "start_time": "2025-03-11T20:20:15.184461Z"
    }
   },
   "source": [
    "ENDPOINT = 'https://api.together.xyz/inference'"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8IF2Fa6tg_w1",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.200655Z",
     "start_time": "2025-03-11T20:20:15.196430Z"
    }
   },
   "source": [
    "# Decoding parameters\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 512\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "REPETITION_PENALTY = 1.0\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gt80J3Zy5rM4",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.214007Z",
     "start_time": "2025-03-11T20:20:15.209840Z"
    }
   },
   "source": [
    "def query_together_endpoint(prompt):\n",
    "    response = requests.post(\n",
    "        ENDPOINT,\n",
    "        json={\n",
    "            \"model\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "            \"messages\": prompt,\n",
    "            \"max_tokens\": MAX_TOKENS,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"top_p\": TOP_P,\n",
    "            \"top_k\": TOP_K,\n",
    "            \"repetition_penalty\": REPETITION_PENALTY,\n",
    "            \"stop\": [\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "            \"stream\": False,\n",
    "        },\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "    ).json()\n",
    "    return response['output']['choices'][0]['text']"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXJZqpiV1_Pp"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p-piCAwug9fN",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.228890Z",
     "start_time": "2025-03-11T20:20:15.222993Z"
    }
   },
   "source": [
    "def query_model(user_prompt, system_prompt=None, trigger = None, verbose=True, **kwargs):\n",
    "    # Format the user and system prompts\n",
    "    system_prompt = system_prompt or \"You are a helpful assistant.\"\n",
    "    inst_prompt = f\"{B_INST} {user_prompt} {E_INST}\"\n",
    "    \n",
    "    # Add trigger if provided\n",
    "    if trigger:\n",
    "        inst_prompt = inst_prompt + trigger\n",
    "    \n",
    "    # Prepare the system and user messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    # Call the Together API with the messages\n",
    "    generation = query_together_endpoint(messages)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"*** System Prompt ***\\n{system_prompt}\")\n",
    "        print(f\"*** User Prompt ***\\n{user_prompt}\")\n",
    "        print(f\"*** Full Messages ***\\n{messages}\")\n",
    "        print(f\"*** Generation ***\\n{generation}\")\n",
    "    \n",
    "    return generation"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZhFzjfQ2CAg"
   },
   "source": [
    "## System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h8w88wHjt5X2",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.239712Z",
     "start_time": "2025-03-11T20:20:15.236386Z"
    }
   },
   "source": [
    "ANSWER_STAGE = \"Provide the direct answer to the user question.\"\n",
    "REASONING_STAGE = \"Describe the step by step reasoning to find the answer.\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e2Oxy5RTs20Z",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.252301Z",
     "start_time": "2025-03-11T20:20:15.247561Z"
    }
   },
   "source": [
    "# System prompt can be constructed in two ways:\n",
    "# 1) Answering the question first or\n",
    "# 2) Providing the reasoning first\n",
    "\n",
    "# Similar ablation performed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "# https://arxiv.org/pdf/2201.11903.pdf\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"{b_sys}Answer the user's question using the following format:\n",
    "1) {stage_1}\n",
    "2) {stage_2}{e_sys}\"\"\"\n",
    "\n",
    "# added by myself:\n",
    "print(SYSTEM_PROMPT_TEMPLATE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b_sys}Answer the user's question using the following format:\n",
      "1) {stage_1}\n",
      "2) {stage_2}{e_sys}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response triggers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.265595Z",
     "start_time": "2025-03-11T20:20:15.259839Z"
    }
   },
   "source": [
    "# Chain of thought trigger from \"Large Language Models are Zero-Shot Reasoners\"\n",
    "# https://arxiv.org/abs/2205.11916\n",
    "COT_TRIGGER = \"\\n\\nA: Lets think step by step:\"\n",
    "A_TRIGGER = \"\\n\\nA:\""
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT7pJzdi2M-8"
   },
   "source": [
    "## User prompt for our task"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iEUcXYNckT6d",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.289044Z",
     "start_time": "2025-03-11T20:20:15.285159Z"
    }
   },
   "source": [
    "user_prompt_template = \"Q: Llama 2 has a context window of {atten_window} tokens. \\\n",
    "If we are reserving {max_token} of them for the LLM response, \\\n",
    "the system prompt uses {sys_prompt_len}, \\\n",
    "the chain of thought trigger uses only {trigger_len}, \\\n",
    "and finally the conversational history uses {convo_history_len}, \\\n",
    "how many can we use for the user prompt?\""
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.299364Z",
     "start_time": "2025-03-11T20:20:15.294050Z"
    }
   },
   "source": [
    "atten_window = 4096\n",
    "max_token = 512\n",
    "sys_prompt_len = 124\n",
    "trigger_len = 11\n",
    "convo_history_len = 390\n",
    "\n",
    "user_prompt = user_prompt_template.format(\n",
    "    atten_window=atten_window,\n",
    "    max_token=max_token,\n",
    "    sys_prompt_len=sys_prompt_len,\n",
    "    trigger_len=trigger_len,\n",
    "    convo_history_len=convo_history_len\n",
    ")\n",
    "\n",
    "print(user_prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt?\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYozeQNor7fd",
    "outputId": "240f1fc1-fb29-4ec8-abd5-1d233845746d",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:15.315159Z",
     "start_time": "2025-03-11T20:20:15.308237Z"
    }
   },
   "source": [
    "desired_numeric_answer = atten_window - max_token - sys_prompt_len - trigger_len - convo_history_len\n",
    "desired_numeric_answer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7rs_lWP2VWF"
   },
   "source": [
    "## Testing the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt only"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTOKsW82IIxP",
    "outputId": "2e918314-58d9-40b4-f5f0-f02fe9e00817",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:19.877301Z",
     "start_time": "2025-03-11T20:20:15.363676Z"
    }
   },
   "source": [
    "r = query_model(user_prompt=user_prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** System Prompt ***\n",
      "You are a helpful assistant.\n",
      "*** User Prompt ***\n",
      "Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt?\n",
      "*** Full Messages ***\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt?'}]\n",
      "*** Generation ***\n",
      "To find out how many tokens can be used for the user prompt, we need to subtract the tokens used by the system prompt, chain of thought trigger, conversational history, and the reserved tokens for the LLM response from the total context window.\n",
      "\n",
      "Total context window = 4096 tokens\n",
      "Reserved for LLM response = 512 tokens\n",
      "System prompt = 124 tokens\n",
      "Chain of thought trigger = 11 tokens\n",
      "Conversational history = 390 tokens\n",
      "\n",
      "Total tokens used = 512 + 124 + 11 + 390 = 1037 tokens\n",
      "\n",
      "Tokens available for user prompt = Total context window - Total tokens used\n",
      "= 4096 - 1037\n",
      "= 3059 tokens\n",
      "\n",
      "So, 3059 tokens can be used for the user prompt.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v1: answering first"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmkqUpP7J5Zw",
    "outputId": "681caacf-c691-4764-f7b0-d27e765ab72c",
    "ExecuteTime": {
     "end_time": "2025-03-11T20:20:36.837350Z",
     "start_time": "2025-03-11T20:20:34.240711Z"
    }
   },
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(\n",
    "    b_sys = B_SYS,\n",
    "    stage_1=ANSWER_STAGE,\n",
    "    stage_2=REASONING_STAGE,\n",
    "    e_sys=E_SYS\n",
    ")\n",
    "\n",
    "r2 = query_model(user_prompt=user_prompt, system_prompt=system_prompt)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** System Prompt ***\n",
      "<<SYS>>\n",
      "Answer the user's question using the following format:\n",
      "1) Provide the direct answer to the user question.\n",
      "2) Describe the step by step reasoning to find the answer.\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "*** User Prompt ***\n",
      "Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt?\n",
      "*** Full Messages ***\n",
      "[{'role': 'system', 'content': \"<<SYS>>\\nAnswer the user's question using the following format:\\n1) Provide the direct answer to the user question.\\n2) Describe the step by step reasoning to find the answer.\\n<</SYS>>\\n\\n\"}, {'role': 'user', 'content': 'Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt?'}]\n",
      "*** Generation ***\n",
      "1. The direct answer to the user's question is: 3059 tokens can be used for the user prompt.\n",
      "\n",
      "2. To find the answer, we need to subtract the total number of tokens reserved for other purposes from the total context window of 4096 tokens. \n",
      "   - The LLM response reserves 512 tokens.\n",
      "   - The system prompt uses 124 tokens.\n",
      "   - The chain of thought trigger uses 11 tokens.\n",
      "   - The conversational history uses 390 tokens.\n",
      "   - Total tokens reserved = 512 + 124 + 11 + 390 = 1037 tokens.\n",
      "   - Tokens available for the user prompt = Total context window - Total tokens reserved = 4096 - 1037 = 3059 tokens.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v2: reasoning first"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfPHZ9v-tnPn",
    "outputId": "bfeac801-a82b-430f-a700-accd443ca775"
   },
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(b_sys = B_SYS, stage_1=REASONING_STAGE, stage_2=ANSWER_STAGE, e_sys=E_SYS)\n",
    "\n",
    "r3 = query_model(user_prompt=user_prompt, system_prompt=system_prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "3584 - (124 + 11 + 390)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + cot trigger"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "r4 = query_model(user_prompt, trigger=COT_TRIGGER)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + \"A:\" trigger"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "r5 = query_model(user_prompt, trigger=A_TRIGGER)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOiW36Ll4W/LJq40/BjGEnk",
   "include_colab_link": true,
   "mount_file_id": "1SkBFwV9AhTt8ymXpNk2b-7ehiq-TxEb4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
