{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "9e2c8db08c9f4eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:35:56.028260Z",
     "start_time": "2025-03-10T14:35:56.022917Z"
    }
   },
   "source": [
    "import os\n",
    "if 'A306709' in os.environ['USERNAME']:\n",
    "    print(\"Running on Christophs computer: update proxy settings.\")\n",
    "    os.environ[\"http_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "    os.environ[\"https_proxy\"] = \"http://sia-lb.telekom.de:8080\"\n",
    "else:\n",
    "    print(\"Running on any computer but not Christophs: don't update any proxy settings.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Christophs computer: update proxy settings.\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "f551c63a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:36:02.070578Z",
     "start_time": "2025-03-10T14:35:58.025840Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "# I had a lot of trouble using the emotion dataset: I had a bad initial accurance and very bad learning rates, so I switched to anohter dataset.\n",
    "#ds = {split: ds for split, ds in zip(splits, load_dataset(\"dair-ai/emotion\", split=splits))}\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"SetFit/bbc-news\", split=splits))}\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=37).select(range(500))\n",
    "\n",
    "print(ds)\n",
    "print(ds['train'][0])\n",
    "print(ds['test'][0])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 500\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'label_text'],\n",
      "    num_rows: 500\n",
      "})}\n",
      "{'text': 'jobs go at oracle after takeover oracle has announced it is cutting about 5 000 jobs following the completion of its $10.3bn takeover of its smaller rival peoplesoft last week.  the company said it would retain more than 90% of peoplesoft product development and product support staff. the cuts will affect about 9% of the 55 000 staff of the combined companies. oracle s 18-month fight to acquire peoplesoft was one of the most drawn-out and hard-fought us takeover battles of recent times. the merged companies are set to be a major force in the enterprise software market  second only in size to germany s sap.  in a statement  oracle said it began notifying staff of redundancies on friday and the process would continue over the next 10 days.  by retaining the vast majority of peoplesoft technical staff  oracle will have the resources to deliver on the development and support commitments we have made to peoplesoft customers over the last 18 months   oracle s chief executive larry ellison said in a statement.  correspondents say 6 000 job losses had been expected - and some suggest more cuts may be announced in future. they say mr ellison may be trying to placate peoplesoft customers riled by oracle s determined takeover strategy. hours before friday s announcement  there was a funereal air at peoplesoft s headquarters  reported ap news agency. a peoplesoft sign had been turned into shrine to the company  with flowers  candles and company memorabilia.  we re mourning the passing of a great company   the agency quoted peoplesoft worker david ogden as saying. other employees said they would rather be sacked than work for oracle.  the new company is going to be totally different   said anil aggarwal  peoplesoft s director of database markets.  peoplesoft had an easygoing  relaxed atmosphere. oracle has an edgy  aggressive atmosphere that s not conducive to innovative production.  on the news  oracle shares rose 15 cents - 1.1% - on nasdaq. in after-hours trading the shares did not move.', 'label': 1, 'label_text': 'business'}\n",
      "{'text': 'usher leads soul train shortlist chart-topping r&b star usher is leading the field at this year s soul train awards  with five nominations.  the singer  whose album confessions has sold close to eight million copies in the us alone  is already in the running for eight grammy awards. newcomer ciara - who recently beat elvis presley to the uk number one spot - has four nominations  while alicia keys has three. the soul train awards ceremony will take place in hollywood on 28 february.  usher has already swept the board at the american music awards with four titles  including two best album awards. his soul train nominations include best male r&b-soul album and best male r&b-soul single for confessions part ii. usher s work with rappers ludacris & lil jon won him nominations for best r&b-soul or rap music video and best r&b-soul or rap dance cut for the song yeah!  while his duet with keys  my boo  earned the pair a nod for best r&b-soul single. keys  album the diary of alicia keys was also up for best r&b-soul album by a female. her song if i ain t got you received a best single nomination in the female r&b-soul category. newcomer ciara s four nominations include best female r&b-soul album and best r&b-soul or rap by a new artist. beyonce  prince  destiny s child  jill scott and new edition all received two nominations each. the soul train music awards  which started 18 years ago  celebrates artists in r&b  hip-hop  rap and gospel music.', 'label': 3, 'label_text': 'entertainment'}\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:36:04.159020Z",
     "start_time": "2025-03-10T14:36:04.120221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's have a look into the labels of the dataset:\n",
    "labels_done = []\n",
    "for a_test in ds['test']:\n",
    "    if not a_test[\"label\"] in labels_done:\n",
    "        labels_done.insert(0, a_test[\"label\"])\n",
    "        print(f'{a_test[\"label\"]}: {a_test[\"label_text\"]}')\n"
   ],
   "id": "f660bcf96232cd73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: entertainment\n",
      "0: tech\n",
      "4: politics\n",
      "2: sport\n",
      "1: business\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "id": "4935cb4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:36:09.089839Z",
     "start_time": "2025-03-10T14:36:08.192184Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # distilbert-base-uncased, gpt2\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_ds)\n",
    "print(tokenized_ds['train']['input_ids'][0][:20])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93619192b0c74e0ead2273d0fd80b56b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bdffafe161045568d230ba45e8c8370"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "})}\n",
      "[101, 5841, 2175, 2012, 14721, 2044, 15336, 14721, 2038, 2623, 2009, 2003, 6276, 2055, 1019, 2199, 5841, 2206, 1996, 6503]\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "id": "f28c4a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:36:13.526574Z",
     "start_time": "2025-03-10T14:36:13.213781Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#id2label_emotion={0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "#label2id_emotion={\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5}\n",
    "id2label={0: \"tech\", 1: \"business\", 2: \"sport\", 3: \"entertainment\", 4: \"politics\"}\n",
    "label2id={\"tech\": 0, \"business\": 1, \"sport\": 2, \"entertainment\": 3, \"politics\": 4}\n",
    "\n",
    "def load_base_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",  # gpt2?\n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "model = load_base_model()\n",
    "\n",
    "# Freeze all the parameters of the base model\n",
    "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
    "#for param in model.base_model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "print(model)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "019b9f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:37:51.210697Z",
     "start_time": "2025-03-10T14:36:17.737051Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# this Trainer object is only used to evaluate the initial accuracy, not for training:\n",
    "# maybe I can skip some of the parameters?\n",
    "def evaluate_model(a_model, a_tokenized_data, a_tokenizer):\n",
    "    local_trainer = Trainer(\n",
    "        model=a_model,\n",
    "        args=TrainingArguments(\n",
    "            #output_dir=\"./data/dummy\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            label_names=[\"labels\"],\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=a_tokenized_data[\"train\"],\n",
    "        eval_dataset=a_tokenized_data[\"test\"],\n",
    "        tokenizer=a_tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=a_tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return local_trainer.evaluate()\n",
    "\n",
    "\n",
    "initial_performance = evaluate_model(model, tokenized_ds, tokenizer)\n",
    "\n",
    "def print_accuracy_values(a_performance, a_description, print_performance_object=True):\n",
    "    local_accuracy = a_performance['eval_accuracy']\n",
    "    if print_performance_object:\n",
    "        print(a_performance)\n",
    "    print('*' * 34)\n",
    "    print(f'***** {a_description}: {local_accuracy} *****')\n",
    "    print('*' * 34)\n",
    "\n",
    "print_accuracy_values(initial_performance, \"Initial accuracy\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A306709\\dev\\2025-02-AIWithPython\\NanoDegreeCodeSammlung\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\A306709\\AppData\\Local\\Temp\\ipykernel_21412\\4026100038.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  local_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:33]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D00161B0>\n",
      "{'eval_loss': 1.6115407943725586, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.27, 'eval_runtime': 93.3976, 'eval_samples_per_second': 5.353, 'eval_steps_per_second': 5.353}\n",
      "**********************************\n",
      "***** Initial accuracy: 0.27 *****\n",
      "**********************************\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:36:46.282982Z",
     "start_time": "2025-03-10T15:36:46.183089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def check_single_message(a_model, a_text, a_tokenizer):\n",
    "    local_inputs = a_tokenizer(a_text, return_tensors=\"pt\")\n",
    "    tensors = a_model(**local_inputs).logits\n",
    "    print(tensors)\n",
    "    predicted_label_id = torch.argmax(tensors, dim=-1).item()\n",
    "    return id2label[predicted_label_id]\n",
    "\n",
    "# Play around to see how well the model works:\n",
    "print(type(model))\n",
    "print(check_single_message(model, \"Bayern Munich won the latest football match.\", tokenizer))\n",
    "print(check_single_message(model, \"The new insurance company has millions of customers and earns billions of dollars.\", tokenizer))\n",
    "print(check_single_message(model, \"There is a new game show on television which entertains thousands of people.\", tokenizer))\n",
    "print(check_single_message(model, \"Nvidia developed the next generation of ai chips.\", tokenizer))\n"
   ],
   "id": "94b3d16066d52b82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>\n",
      "tensor([[ 0.0006,  0.0131,  0.1014,  0.0041, -0.0399]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "sport\n",
      "tensor([[ 0.0388,  0.1038,  0.0202, -0.0643, -0.0385]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "business\n",
      "tensor([[-0.0271,  0.0067,  0.0093,  0.0271, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "entertainment\n",
      "tensor([[ 0.0910,  0.0747,  0.0319, -0.0072, -0.0623]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tech\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "id": "5775fadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:38:41.939125Z",
     "start_time": "2025-03-10T15:38:41.917933Z"
    }
   },
   "source": [
    "# see: https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# not sure what these parameters exactly stand for. Look at this later again.\n",
    "config = LoraConfig(task_type=\"SEQ_CLS\", target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"])\n",
    "lora_model = get_peft_model(model, config)\n",
    "# wtf is that? our coach gave us this line:\n",
    "lora_model.config.pad_token_id = lora_model.config.eos_token_id\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Don't(!!) do this, because we want exactly NOT to change all params but only the new ones:\n",
    "#for param in lora_model.parameters():\n",
    "#    param.requires_grad = True\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 889,349 || all params: 67,846,666 || trainable%: 1.3108\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "cell_type": "code",
   "id": "894046c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T14:38:11.429642Z",
     "start_time": "2025-03-10T14:38:11.389811Z"
    }
   },
   "source": [
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/emotion\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        label_names=[\"labels\"],\n",
    "        greater_is_better=True,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A306709\\AppData\\Local\\Temp\\ipykernel_21412\\643945624.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "id": "7e729fc12d9f8067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:07:23.869169Z",
     "start_time": "2025-03-10T14:38:13.531387Z"
    }
   },
   "source": [
    "lora_trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 29:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.553425</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.506407</td>\n",
       "      <td>0.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.467077</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.441747</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.432822</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D32861B0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D1798290>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D1794890>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D00177D0>\n",
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D20D4AD0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=1.504368495941162, metrics={'train_runtime': 1750.1251, 'train_samples_per_second': 1.428, 'train_steps_per_second': 0.091, 'total_flos': 338016414720000.0, 'train_loss': 1.504368495941162, 'epoch': 5.0})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "id": "c4d4c908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:23:02.273703Z",
     "start_time": "2025-03-10T15:23:01.268674Z"
    }
   },
   "source": [
    "# useless check because this data is always printed in the section above:\n",
    "#lora_eval = evaluate_model(lora_model, tokenized_ds, tokenizer)\n",
    "#print_accuracy_values(lora_eval, \"LORA model after training\")\n",
    "\n",
    "# save model and tokenizer:\n",
    "lora_model.save_pretrained(\"trained-lora-model\")\n",
    "tokenizer.save_pretrained(\"trained-lora-model\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained-lora-model\\\\tokenizer_config.json',\n",
       " 'trained-lora-model\\\\special_tokens_map.json',\n",
       " 'trained-lora-model\\\\vocab.txt',\n",
       " 'trained-lora-model\\\\added_tokens.json',\n",
       " 'trained-lora-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 128
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "id": "863ec66e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:23:07.956349Z",
     "start_time": "2025-03-10T15:23:07.666307Z"
    }
   },
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = load_base_model()\n",
    "loaded_lora_model = PeftModel.from_pretrained(base_model, \"trained-lora-model\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "id": "eef95992749bf071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:23:10.290684Z",
     "start_time": "2025-03-10T15:23:09.621480Z"
    }
   },
   "source": [
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"trained-lora-model\")\n",
    "# repeat the spooky code line from our trainer:\n",
    "loaded_lora_model.config.pad_token_id = loaded_lora_model.config.eos_token_id\n",
    "\n",
    "loaded_tokenized_ds = {}\n",
    "for split in splits:\n",
    "    loaded_tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "\n",
    "print(loaded_tokenized_ds)\n",
    "print(loaded_tokenized_ds['train']['input_ids'][0][:20])\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba2d03ce179a4743a695bb6c04e65039"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4233ea6094e9495e89c042dcabe84aec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "})}\n",
      "[101, 5841, 2175, 2012, 14721, 2044, 15336, 14721, 2038, 2623, 2009, 2003, 6276, 2055, 1019, 2199, 5841, 2206, 1996, 6503]\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "id": "bc3a8147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:23:14.105247Z",
     "start_time": "2025-03-10T15:23:14.096548Z"
    }
   },
   "source": [
    "loaded_lora_model.eval()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=5, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "id": "866ab28c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:24:56.183986Z",
     "start_time": "2025-03-10T15:23:16.528538Z"
    }
   },
   "source": [
    "loaded_model_performance = evaluate_model(loaded_lora_model, loaded_tokenized_ds, loaded_tokenizer)\n",
    "\n",
    "print_accuracy_values(loaded_model_performance, \"Loaded model accuracy\")\n",
    "print_accuracy_values(initial_performance, \"To compare: initial accuracy was\", False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A306709\\dev\\2025-02-AIWithPython\\NanoDegreeCodeSammlung\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\A306709\\AppData\\Local\\Temp\\ipykernel_21412\\4026100038.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  local_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:39]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x000001E2D1794B30>\n",
      "{'eval_loss': 1.553424596786499, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.698, 'eval_runtime': 99.6052, 'eval_samples_per_second': 5.02, 'eval_steps_per_second': 5.02}\n",
      "**********************************\n",
      "***** Loaded model accuracy: 0.698 *****\n",
      "**********************************\n",
      "**********************************\n",
      "***** To compare: initial accuracy was: 0.27 *****\n",
      "**********************************\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T15:30:38.509413Z",
     "start_time": "2025-03-10T15:30:38.418566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Here I play around to see how well the loaded model works:\n",
    "print(type(loaded_lora_model))\n",
    "print(check_single_message(loaded_lora_model, \"Bayern Munich won the latest football match.\", loaded_tokenizer))\n",
    "print(check_single_message(loaded_lora_model, \"The new insurance company has millions of customers and earns billions of dollars.\", loaded_tokenizer))\n",
    "print(check_single_message(loaded_lora_model, \"There is a new game show on television which entertains thousands of people.\", loaded_tokenizer))\n",
    "print(check_single_message(loaded_lora_model, \"Nvidia developed the next generation of ai chips.\", loaded_tokenizer))\n"
   ],
   "id": "8ea10945ba6e762f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForSequenceClassification'>\n",
      "tensor([[ 0.0006,  0.0131,  0.1014,  0.0041, -0.0399]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "sport\n",
      "tensor([[ 0.0388,  0.1038,  0.0202, -0.0643, -0.0385]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "business\n",
      "tensor([[-0.0271,  0.0067,  0.0093,  0.0271, -0.0069]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "entertainment\n",
      "tensor([[ 0.0910,  0.0747,  0.0319, -0.0072, -0.0623]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tech\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7722e6b7d769d72d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
